{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import jax.numpy as jnp\n",
    "import jax.random as jrandom\n",
    "import numpy as np\n",
    "from jax import grad\n",
    "from jax import jit, grad, jacfwd, jacrev\n",
    "import math\n",
    "\n",
    "\n",
    "import scipy \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = [15,15]\n",
    "plt.style.use('default')\n",
    "plt.rcParams['lines.markersize'] = 8\n",
    "plt.rcParams.update({'font.size': 12})\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "from jax.config import config\n",
    "config.update(\"jax_enable_x64\", True)\n",
    "\n",
    "import sys\n",
    "HOME = \"/Users/daniellengyel/curr_adventure/exact_sampling\"\n",
    "sys.path.append(HOME)\n",
    "\n",
    "from Ours import Ours\n",
    "from FD import FD\n",
    "from Functions import Quadratic, PyCutestGetter\n",
    "from RBF import RBF\n",
    "\n",
    "from scipy.interpolate import Rbf, RBFInterpolator\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "func_name = \"ARWHEAD\"\n",
    "dim = 100\n",
    "sig = 0.01\n",
    "_, x_init, F = PyCutestGetter(func_name=func_name, func_dim=dim, func_i=None, dim_i=None, sig=0, noise_type=\"gaussian\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "297.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.f(jnp.ones(dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ExactH_GD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "from jax import grad\n",
    "from jax import random as jrandom\n",
    "import time\n",
    "from tqdm import tqdm \n",
    "\n",
    "from jax import jit, grad, jacfwd\n",
    "from scipy.interpolate import Rbf, RBFInterpolator\n",
    "\n",
    "\n",
    "from jax.config import config\n",
    "config.update(\"jax_enable_x64\", True)\n",
    "\n",
    "class OptimizationBlueprint:\n",
    "    def __init__(self, x_init, F, step_size, num_total_steps, sig, jrandom_key, grad_eps=0, verbose=False, x_opt=None):\n",
    "        self.jrandom_key = jrandom_key\n",
    "\n",
    "        self.step_size = step_size\n",
    "\n",
    "        self.sig = sig\n",
    "\n",
    "        self.F = F\n",
    "\n",
    "        self.loop_steps_remaining = num_total_steps\n",
    "        self.num_total_steps = num_total_steps\n",
    "        self.grad_eps = grad_eps\n",
    "\n",
    "        self.verbose = True\n",
    "        self.x_init = x_init\n",
    "        self.dim = len(x_init)\n",
    "        self.verbose = verbose\n",
    "\n",
    "        self.x_opt = x_opt\n",
    "         \n",
    "\n",
    "    def run_opt(self):\n",
    "        X = self.x_init\n",
    "\n",
    "        vals_arr = []\n",
    "        x_arr = []\n",
    "        total_func_calls = 0\n",
    "        start_time = time.time()\n",
    "\n",
    "        vals_arr.append((self.F.f(X), time.time() - start_time, total_func_calls, 0))\n",
    "        x_arr.append(X)\n",
    "\n",
    "        for t in tqdm(range(self.loop_steps_remaining)):\n",
    "\n",
    "            \n",
    "            # get search direction\n",
    "            if self.jrandom_key is not None:\n",
    "                self.jrandom_key, subkey = jrandom.split(self.jrandom_key)\n",
    "                search_direction, f1, num_func_calls = self.step_getter(X, subkey)\n",
    "            else:\n",
    "                search_direction, f1, num_func_calls = self.step_getter(X)\n",
    "            total_func_calls += num_func_calls\n",
    "\n",
    "            vals_arr.append((self.F.f(X), time.time() - start_time, total_func_calls, float(jnp.linalg.norm(f1 - self.F.f1(X))))) # jnp.linalg.norm(X - self.F.x_opt))) #float(f1.T @ self.F.f1(X)) / (jnp.linalg.norm(self.F.f1(X)) * jnp.linalg.norm(f1))))#  float(jnp.linalg.norm(f1 - self.F.f1(X)))/jnp.linalg.norm(self.F.f1(X))))# jnp.linalg.norm(X - self.x_init)))# # jnp.linalg.norm(self.F.f1(X)))) # #float(jnp.linalg.norm(self.grad_curr - self.F.f1(X))/jnp.linalg.norm(self.F.f1(X))))) #/jnp.linalg.norm(self.F.f1(X))))) jnp.linalg.norm(alpha * search_direction))) #\n",
    "            x_arr.append(X)\n",
    "\n",
    "            if jnp.linalg.norm(f1)/self.dim < self.grad_eps:\n",
    "                break\n",
    "\n",
    "            if self.verbose:\n",
    "                # print(X)\n",
    "                print(\"Number Iterations\", t)\n",
    "                print(\"Num Function Calls\", total_func_calls)\n",
    "                print(\"Obj\", self.F.f(X))\n",
    "                print(\"Grad norm\", jnp.linalg.norm(f1))\n",
    "                print(\"True Norm\", jnp.linalg.norm(self.F.f1(X)))\n",
    "                print()\n",
    "\n",
    "            if self.F.f(X) == float(\"inf\"):\n",
    "                break\n",
    "            \n",
    "            # update step\n",
    "            X = X + self.step_size * search_direction\n",
    "\n",
    "            if vals_arr[-1][-1] > 1e10:\n",
    "                break\n",
    "\n",
    "        return X, jnp.array(vals_arr), jnp.array(x_arr)\n",
    "\n",
    "\n",
    "    def step_getter(self, X, jrandom_key=None):\n",
    "        pass\n",
    "\n",
    "\n",
    "class NewtonMethod(OptimizationBlueprint):\n",
    "    def __init__(self, x_init, F, step_size, num_total_steps, sig, jrandom_key, grad_eps=0, verbose=False):\n",
    "        super().__init__(x_init, F, step_size, num_total_steps, sig, jrandom_key, grad_eps, verbose)\n",
    "        self.sig = 0\n",
    "        self.grad_curr = None\n",
    "    \n",
    "    def step_getter(self, X, jrandom_key=None):\n",
    "        f1 = self.F.f1(X)\n",
    "        f2 = self.F.f2(X)\n",
    "        self.grad_curr = f1\n",
    "        return -jnp.linalg.inv(f2).dot(f1), f1, 2\n",
    "\n",
    "class GradientDescent(OptimizationBlueprint):\n",
    "    def __init__(self, x_init, F, step_size, num_total_steps, sig, jrandom_key, grad_eps=0, verbose=False):\n",
    "        super().__init__(x_init, F, step_size, num_total_steps, sig, jrandom_key, grad_eps, verbose)\n",
    "        self.sig = 0\n",
    "        self.grad_curr = None\n",
    "    \n",
    "    def step_getter(self, X, jrandom_key=None):\n",
    "        f1 = self.F.f1(X)\n",
    "        self.grad_curr = f1\n",
    "        return -f1, f1, 2 \n",
    "\n",
    "\n",
    "class ExactH_GD(OptimizationBlueprint):\n",
    "    def __init__(self, x_init, F, step_size, num_total_steps, sig, jrandom_key, grad_getter, grad_eps=0, verbose=False):\n",
    "        super().__init__(x_init, F, step_size, num_total_steps, sig, jrandom_key, grad_eps, verbose)\n",
    "\n",
    "        self.grad_getter = grad_getter\n",
    "        \n",
    "\n",
    "    def step_getter(self, X, jrandom_key):\n",
    "        num_func_calls = 0\n",
    "        \n",
    "        H = self.F.f2(X)\n",
    "        f1, num_func_calls, _, _, _ = self.grad_getter.grad(self.F, X, jrandom_key, H=H)\n",
    "\n",
    "        if self.verbose:\n",
    "            print(\"Grad diff\", jnp.linalg.norm(f1 - self.F.f1(X))/jnp.linalg.norm(self.F.f1(X)))\n",
    "\n",
    "        return -f1, f1, num_func_calls\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class InterpH_GD(OptimizationBlueprint):\n",
    "    def __init__(self, x_init, F, step_size, num_total_steps, sig, jrandom_key, grad_getter, grad_eps=0, verbose=False, smoothing=1):\n",
    "        super().__init__(x_init, F, step_size, num_total_steps, sig, jrandom_key, grad_eps, verbose)\n",
    "\n",
    "\n",
    "        self.X_prev = None\n",
    "        self.grad_curr = None\n",
    "        self.grad_getter = grad_getter\n",
    "        \n",
    "        self.interp_points = None\n",
    "        self.F_vals = None\n",
    "        \n",
    "        self.smoothing = smoothing\n",
    "        \n",
    "\n",
    "    def step_getter(self, X, jrandom_key):\n",
    "        num_func_calls = 0\n",
    "        if (self.interp_points is not None) and (self.interp_points.shape[1] > 2 * len(X) + 1):\n",
    "            curr_interp_points = []\n",
    "            curr_F_vals = []\n",
    "            delta_up = 5\n",
    "            delta_low = 1e-9\n",
    "            # for i in range(self.interp_points.shape[1]):\n",
    "            #     dist = jnp.linalg.norm(self.interp_points[:, i] - X)\n",
    "            #     if (dist < delta_up) and (dist > delta_low):\n",
    "            #         curr_interp_points.append(self.interp_points[:, i])\n",
    "            #         curr_F_vals.append(self.F_vals[i])\n",
    "\n",
    "            curr_interp_points = self.interp_points[:, -1000:] # int(-self.dim**2 * 2):]# self.interp_points # jnp.array(curr_interp_points).T # \n",
    "            curr_F_vals = self.F_vals[-1000:] # [-int(self.dim**2 * 2): ] # self.F_vals # jnp.array(curr_F_vals) # \n",
    "            # print(\"num F sub\", len(curr_F_vals))\n",
    "\n",
    "            # curr_interp_points = curr_interp_points[:, -int(self.dim**2/8):]\n",
    "            # curr_F_vals = curr_F_vals[-int(self.dim**2/8):]\n",
    "\n",
    "            # print(\"num F sub\", len(curr_F_vals))\n",
    "            # print(\"F sub all\", len(self.F_vals))\n",
    "            # print()\n",
    "\n",
    "            # if curr_interp_points.shape[1] < 2 * len(X) + 1:\n",
    "            #     H = jnp.eye(len(X))\n",
    "            # else:\n",
    "            #     H = self.get_G(curr_interp_points, curr_F_vals)\n",
    "\n",
    "            # H = self.get_G(curr_interp_points, curr_F_vals)\n",
    "            H, rbf_f1 = self.get_H_rbf(X, curr_interp_points, curr_F_vals)\n",
    "            # print(repr(H))\n",
    "            # print(\"H diff\", jnp.linalg.norm(H - self.F.f2(X))/jnp.linalg.norm(self.F.f2(X)))\n",
    "\n",
    "        else:\n",
    "            H = jnp.eye(len(X))\n",
    "            rbf_f1 = None\n",
    "\n",
    "        self.grad_curr, num_func_calls, F_x_0, FS, S = self.grad_getter.grad(self.F, X, jrandom_key, H=H)\n",
    "\n",
    "        if self.interp_points is None:\n",
    "            self.interp_points = jnp.concatenate([S + X.reshape(-1, 1), X.reshape(-1, 1)], axis=1)\n",
    "            self.F_vals = jnp.concatenate([FS, jnp.array([F_x_0])])\n",
    "        else:\n",
    "            self.interp_points = jnp.concatenate([self.interp_points, S + X.reshape(-1, 1), X.reshape(-1, 1)], axis=1)\n",
    "            self.F_vals = jnp.concatenate([self.F_vals, FS, jnp.array([F_x_0])])\n",
    "        \n",
    "        if self.verbose:\n",
    "\n",
    "            print(\"Grad diff\", jnp.linalg.norm(self.grad_curr - self.F.f1(X))/jnp.linalg.norm(self.F.f1(X)))\n",
    "            print(\"H diff\", jnp.linalg.norm(H - self.F.f2(X))/jnp.linalg.norm(self.F.f2(X)))\n",
    "            # print(\"H eigs\", (jnp.linalg.eigh(H)[0] - jnp.linalg.eigh(self.F.f2(X))[0])/jnp.linalg.eigh(self.F.f2(X))[0])\n",
    "            \n",
    "        f1 = self.grad_curr\n",
    "\n",
    "        return -f1, f1, num_func_calls\n",
    "    \n",
    "    def create_W(self, S):\n",
    "        dim = S.shape[0]\n",
    "        N = S.shape[1]\n",
    "        A = 1/2 * (S.T @ S)**2\n",
    "        eXT = jnp.concatenate([jnp.ones(shape=(N, 1)), S.T], axis=1)\n",
    "        eTX0 = jnp.concatenate([eXT.T, jnp.zeros(shape=(dim+1, dim+1))], axis=1)\n",
    "        W = jnp.concatenate([A, eXT], axis=1)\n",
    "        W = jnp.concatenate([W, eTX0], axis=0)\n",
    "        return W\n",
    "    \n",
    "    def get_G(self, S, F_vals):\n",
    "        dim = len(S)\n",
    "        \n",
    "        W = self.create_W(S)\n",
    "        F_vals0 = jnp.concatenate([F_vals, jnp.zeros(dim + 1)])\n",
    "        lcg = jnp.linalg.solve(W, F_vals0)\n",
    "\n",
    "        lmbda = lcg[:-(dim+1)]\n",
    "\n",
    "        return S @ jnp.diag(lmbda) @ S.T\n",
    "    \n",
    "    def get_H_rbf(self, x_0, S, F_vals):\n",
    "\n",
    "        rbf = RBFInterpolator(S.T, F_vals, smoothing=self.smoothing) #, epsilon=0.1, kernel=\"gaussian\")\n",
    "        coeffs = jnp.array(rbf._coeffs)\n",
    "        y = jnp.array(rbf.y)\n",
    "        epsilon = rbf.epsilon\n",
    "        shift = jnp.array(rbf._shift)\n",
    "        scale = jnp.array(rbf._scale)\n",
    "        powers = jnp.array(rbf.powers)\n",
    "\n",
    "        H = self._thin_plate_f2(x_0, y, epsilon, coeffs, shift, scale, powers)\n",
    "\n",
    "        # rbf_f1 = grad(lambda x: self._evaluate(x, y, epsilon, coeffs, shift, scale, powers))\n",
    "        # rbf_f2 = jacfwd(lambda x: rbf_f1(x))\n",
    "        # print(\"H diff approx\", jnp.linalg.norm(H - rbf_f2(x_0)))\n",
    "\n",
    "        f1 = None # rbf_f1(jnp.array(x_0))\n",
    "\n",
    "\n",
    "        return H, f1\n",
    "\n",
    "    def _thin_plate_f2(self, x, y, epsilon, coeffs, shift, scale, powers):\n",
    "        dim = len(x)\n",
    "        p = y.shape[0]\n",
    "        yeps = y*epsilon\n",
    "        xeps = x*epsilon\n",
    "        r = jnp.linalg.norm(xeps - yeps, axis=1)\n",
    "\n",
    "        log_r = jnp.log(r)\n",
    "\n",
    "        a = 2 * epsilon**2 * jnp.eye(dim) * (log_r @ coeffs[:p, 0])\n",
    "        b = 2 * epsilon**2 * (xeps - yeps).T @ ((coeffs[:p, 0]/r**2).reshape(-1, 1) * (xeps - yeps))\n",
    "        c = 2 * epsilon * jnp.eye(dim) * jnp.sum(coeffs[:p, 0])\n",
    "\n",
    "        return a + b + c\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (simplex_grad)",
   "language": "python",
   "name": "simplex_grad"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
