{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "import jax.random as jrandom\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from jax import jit, grad, jacfwd, jacrev\n",
    "\n",
    "\n",
    "\n",
    "from scipy.stats import linregress\n",
    "\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from jax.config import config\n",
    "config.update(\"jax_enable_x64\", True)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = [12,12]\n",
    "plt.style.use('ggplot')\n",
    "plt.rcParams['lines.markersize'] = 8\n",
    "plt.rcParams.update({'font.size': 12})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to minimize the following \n",
    "$$ \n",
    "    tr(\\Sigma V^T D V \\Sigma)\n",
    "$$\n",
    "with respect to $V$ with $V$ being an orthogonal matrix.\n",
    "\n",
    "Differentiating with respect to $V$ we have \n",
    "\n",
    "$$\n",
    "    \\nabla_V tr(\\Sigma V^T D V \\Sigma) \\\\\n",
    "    = 2 D V \\Sigma^2.\n",
    "$$\n",
    "    \n",
    "    \n",
    "Now, we also have the condition that the matrix has to be orthogonal $(V^T V) = I$ which leads to the lagrangian \n",
    "\n",
    "$$\n",
    "    \\mathcal{L}(V, \\lambda) = tr(\\Sigma V^T D V \\Sigma) + \\sum_{i = 1}^d \\sum_{j = 1}^d \\lambda_{i, j}(v_i^T v_j - \\delta_{ij}).\n",
    "$$\n",
    "\n",
    "The gradient of the constraint is $V (\\lambda + \\lambda^T)$. Notice here that $(\\lambda + \\lambda^T)$ is symmetric. \n",
    "\n",
    "We get from the KKT conditions that \n",
    "\n",
    "$$\n",
    "    2 D V \\Sigma^2 = V (\\lambda + \\lambda^T) \\\\\n",
    "    V^T D V \\Sigma^2 = \\frac{1}{2} (\\lambda + \\lambda^T)\n",
    "$$\n",
    "and since the RHS is symmetric we have that \n",
    "$$\n",
    "    V^T D V \\Sigma^2 = \\Sigma^2 V^T D V\n",
    "$$\n",
    "\n",
    "implying that $V^T D V$ commutes with $\\Sigma^2$. Diagonalizable matricies which commute are mutually diagonalizable. Hence there exists $P$ orthogonal such that $P^{-1} V^T D V P$ and $P^{-1} \\Sigma^2 P$ are diagonal matricies. \n",
    "\n",
    "Assume such a $P$ exists then we have that for some diagonal matrix $K$ that\n",
    "$$ \n",
    "    P^{-1} \\Sigma^2 P = K \\\\\n",
    "    \\Sigma^2 P K^{-1} = P \\\\\n",
    "    \\rightarrow \\frac{1}{K_i} \\Sigma^2 P_i = P_i \\\\\n",
    "    \\frac{1}{K_i} \\Sigma^2_j P_{ij} = P_{ij}. \\\\\n",
    "$$\n",
    "\n",
    "Since $P$ has to have full rank at least one $P_{ij} \\neq 0$. Assume that $P_{i j} \\neq 0$ for some $j$ then we have\n",
    "$$\n",
    "    K_i = \\Sigma^2_j\n",
    "$$\n",
    "which immidiately implies that for any other $j$ for which $\\Sigma^2_j \\neq K_i$ $P_{ij} = 0$. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = 5\n",
    "D = jnp.diag(jnp.linspace(10, 20, dim))\n",
    "Sigma = jnp.diag(jnp.linspace(1, 10, dim))\n",
    "\n",
    "jrandom_key = jrandom.PRNGKey(0)\n",
    "jrandom_key, subkey = jrandom.split(jrandom_key)\n",
    "V = jrandom.normal(subkey, shape=(dim, dim,))\n",
    "\n",
    "predicted_grad = 2 * D @ V @ Sigma**2\n",
    "\n",
    "def g(V, h):\n",
    "    res = np.zeros(shape=(dim, dim))\n",
    "    for i in range(dim):\n",
    "        for j in range(dim):\n",
    "            h_add = np.zeros(shape=(dim, dim))\n",
    "            h_add[i, j] = h\n",
    "            partial = (jnp.trace(Sigma @ (V + h_add).T @ D @ (V + h_add) @ Sigma) - jnp.trace(Sigma @ (V).T @ D @ (V) @ Sigma))/h\n",
    "            res[i, j] = partial\n",
    "            \n",
    "    return res\n",
    "\n",
    "# print(predicted_grad)\n",
    "\n",
    "# g(V, 0.01)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-2.24155725 -3.41796199 -2.07293438  3.90568685 -0.16045151]\n",
      " [ 2.18186568  1.3422906   3.16194906  6.81276762 -1.24828383]\n",
      " [ 2.64887435  1.72390982  1.81490638 -4.09982664 -1.89533527]\n",
      " [ 0.11279967  1.23804592  1.21735931  2.09486267 -1.05874889]\n",
      " [-0.13607657  1.78528725  0.65198301 -2.30458692 -2.61382842]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-2.24182828, -3.41927897, -2.0733179 ,  3.90614404, -0.15934457],\n",
       "       [ 2.18159466,  1.34097362,  3.16156553,  6.81322481, -1.24717689],\n",
       "       [ 2.64860332,  1.72259284,  1.81452286, -4.09936945, -1.89422833],\n",
       "       [ 0.11252864,  1.23672894,  1.21697579,  2.09531986, -1.05764195],\n",
       "       [-0.1363476 ,  1.78397027,  0.65159949, -2.30412973, -2.61272148]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# jrandom_key, subkey = jrandom.split(jrandom_key)\n",
    "lmbda = jrandom.normal(subkey, shape=(dim, dim,))\n",
    "\n",
    "def g_const(V, h):\n",
    "    res = np.zeros(shape=(dim, dim))\n",
    "    for i in range(dim):\n",
    "        for j in range(dim):\n",
    "            h_add = np.zeros(shape=(dim, dim))\n",
    "            h_add[i, j] = h\n",
    "            partial = (jnp.sum(lmbda * ((V + h_add).T @ (V + h_add) - np.eye(dim))) - jnp.sum(lmbda * (V.T @ V - np.eye(dim))))/h\n",
    "            res[i, j] = partial\n",
    "            \n",
    "    return res\n",
    "\n",
    "predicted_g_const = V @ (lmbda + lmbda.T) \n",
    "\n",
    "print(predicted_g_const)\n",
    "g_const(V, 0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  20.        0.        0.        0.        0.    ]\n",
      " [   0.      264.0625    0.        0.        0.    ]\n",
      " [   0.        0.      907.5       0.        0.    ]\n",
      " [   0.        0.        0.     2102.1875    0.    ]\n",
      " [   0.        0.        0.        0.     4000.    ]]\n"
     ]
    }
   ],
   "source": [
    "def constraint_grad(V, lmbda):\n",
    "    return V @ (lmbda + lmbda.T)\n",
    "\n",
    "def obj_grad(D, V, Sigma):\n",
    "    return 2 * D @ V @ Sigma**2\n",
    "\n",
    "\n",
    "V = jnp.eye(dim)\n",
    "\n",
    "print(obj_grad(D, V, Sigma))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (new_adv)",
   "language": "python",
   "name": "new_adv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
